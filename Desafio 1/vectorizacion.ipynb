{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "shape: (11314, 101631)\n",
      "cantidad de documentos: 11314\n",
      "tamaño del vocabulario (dimensionalidad de los vectores): 101631\n"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer()\n",
    "\n",
    "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
    "y_train = newsgroups_train.target\n",
    "\n",
    "X_test = tfidfvect.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "print(type(X_train))\n",
    "print(f'shape: {X_train.shape}')\n",
    "print(f'cantidad de documentos: {X_train.shape[0]}')\n",
    "print(f'tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetos y Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_limited_strings(text, max_chars=200):\n",
    "    # Imprime hasta 50 caracteres de cada texto en el dataset\n",
    "    print(text[:max_chars])\n",
    "    print('\\n')  \n",
    "    print('-' * max_chars)# Separador para distinguir entre textos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9510\n",
      "I recently had a case of shingles and my doctors wanted to give me\n",
      "intravenous Acyclovir.\n",
      "\n",
      "It was a pain finding IV sites in my arms...can I have some facts about\n",
      "how advantageous it is to give intravenous antibiotics rather than oral?\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(newsgroups_train.data))\n",
    "print(idx)\n",
    "print(newsgroups_train.data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vectorizar documentos. \n",
    "Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
    "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
    "la similaridad según el contenido del texto y la etiqueta de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're right: Thomas, Gonzalez, Sheffield, and Griffey don't even begin\n",
      "to compare with Ripken, Boggs, and Gwynn, so no wonder Alomar gets so\n",
      "much attention.\n",
      "\n",
      "Sandberg got no attention his rookie year because his rookie year was\n",
      "terrible.  So was his sophomore year.\n",
      "\n",
      "National League pitchers are \"much better pitchers\"?  That certainly explains\n",
      "Sheffield's 1993, hm?  Are you confusing \"have ERA's that are 0.40 lower\n",
      "because they don't face DH's\" with \"much better\"?\n"
     ]
    }
   ],
   "source": [
    "idx = 968\n",
    "print(newsgroups_train.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id de documentos similares: [5234 7528 1326 2510 8759]\n",
      "Valores de documentos similares: [0.44382398 0.1898537  0.18803352 0.18744572 0.17540134]\n"
     ]
    }
   ],
   "source": [
    "cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
    "cossim_values = np.sort(cossim)[::-1][1:6]\n",
    "cossim_docs_idx = np.argsort(cossim)[::-1][1:6]\n",
    "\n",
    "print(f'Id de documentos similares: {cossim_docs_idx}')\n",
    "print(f'Valores de documentos similares: {cossim_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase del documento original: rec.sport.baseball\n",
      "Clase de documentos similares: ['rec.sport.baseball', 'rec.sport.baseball', 'rec.sport.baseball', 'rec.sport.baseball', 'rec.sport.hockey']\n"
     ]
    }
   ],
   "source": [
    "target_original = newsgroups_train.target_names[y_train[idx]]\n",
    "cossim_target_list = []\n",
    "for i in cossim_docs_idx:\n",
    "  cossim_target_list.append(newsgroups_train.target_names[y_train[i]])\n",
    "\n",
    "print(f'Clase del documento original: {target_original}')\n",
    "print(f'Clase de documentos similares: {cossim_target_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Número: 5234\n",
      "\n",
      "\n",
      "\n",
      "Sandberg is not particulary known for his stolen bases.  What competition did \n",
      "Alomar have?  Sandberg came in a year after Ripken, and the same year as Boggs,\n",
      "Gwynn, and the other magicians.  So le\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 7528\n",
      "So far Simmons looks like a total idiot.\n",
      "\n",
      "\n",
      "1) Zane Smith should learn how to \"switchpitch\" and return from the DL. I\n",
      "would rather have Zane Smith pitch right handed than have Moeller pitch at all.\n",
      "\n",
      "2)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 1326\n",
      "To all those out there wondering about who holds the record for three\n",
      "homer games ina career, the answer is Johnny Mize in his career with the \n",
      "Cards and the Yanks.  He hit three 6 times.  I am almost\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 2510\n",
      "\n",
      "I know.  You have this fucked up idea that anybody who prefers Alomar\n",
      "to Baerga must be a Jay-Lover and Indian-Hater.  Sorry, you got that\n",
      "one wrong!  I hate the Jays and don't care one way or the ot\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 8759\n",
      "...\n",
      "\n",
      "Please.  Have a care with Phil.  We liked him a lot in Pittsburgh.  He\n",
      "didn't score a lot if you look at his stats last year but he worked his\n",
      "butt off.  It was his speed that created opportuniti\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in cossim_docs_idx:\n",
    "    print(f'Texto Número: {i}')\n",
    "    print_limited_strings(newsgroups_train.data[i], max_chars=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........\n",
      "\n",
      "It looks like the Edmonton Oilers just decided to take a European\n",
      "vacation this spring...\n",
      "\n",
      "Ranford, Tugnutt, Benning, Manson, Smith, Buchberger, and Corson\n",
      "are playing for Canada.\n",
      "\n",
      "Podein and Weight are playing for the US.\n",
      "\n",
      "Is Kravchuk playing for the Russians...I know he had nagging\n",
      "injuries late in the season.\n",
      "\n",
      "Podein is an interesting case...because he was eligible to\n",
      "play in Cape Breton in the AHL playoffs like Kovalev, Zubov,\n",
      "and Andersson...obviously Sather and Pocklington are not\n",
      "the total scrooges everyone makes them out to be...certainly\n",
      "in this case they've massively outclassed Paramount and the\n",
      "New York Rangers.\n"
     ]
    }
   ],
   "source": [
    "idx = 266\n",
    "print(newsgroups_train.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id de documentos similares: [4078 6165 9057  494 7942]\n",
      "Valores de documentos similares: [0.23097329 0.2295853  0.21965255 0.19736223 0.19725845]\n"
     ]
    }
   ],
   "source": [
    "cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
    "cossim_values = np.sort(cossim)[::-1][1:6]\n",
    "cossim_docs_idx = np.argsort(cossim)[::-1][1:6]\n",
    "\n",
    "print(f'Id de documentos similares: {cossim_docs_idx}')\n",
    "print(f'Valores de documentos similares: {cossim_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase del documento original: rec.sport.hockey\n",
      "Clase de documentos similares: ['rec.sport.hockey', 'rec.sport.hockey', 'rec.sport.hockey', 'rec.sport.hockey', 'rec.sport.hockey']\n"
     ]
    }
   ],
   "source": [
    "target_original = newsgroups_train.target_names[y_train[idx]]\n",
    "cossim_target_list = []\n",
    "for i in cossim_docs_idx:\n",
    "  cossim_target_list.append(newsgroups_train.target_names[y_train[i]])\n",
    "\n",
    "print(f'Clase del documento original: {target_original}')\n",
    "print(f'Clase de documentos similares: {cossim_target_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Número: 4078\n",
      "News reports in Toronto say that the Rangers are insisting that\n",
      "Kovalev, Zubov, and Andersson play for Binghampton in the Calder\n",
      "Cup playoffs, rather than return to play for their \"home\" countries\n",
      "in \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 6165\n",
      "I can't believe that the NY Rangers would force its \n",
      "players to go to Binghamtom to play in the AHL playoffs instead\n",
      "of letting them represent their countries in the World Championships.\n",
      "\n",
      "Anderrson an\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 9057\n",
      "\n",
      "Well, since the Oilers didn't win a heck of a lot of games, I'm not sure\n",
      "they deserve and MVP (\"can't win without him\").  However, I'd suggest Kelly\n",
      "Buchberger instead of Dave Manson, who has had a b\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 494\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nice try Deepak, but \"tough Whaler squad\" should have clued you in to the\n",
      "fact that my Leaf woofing was tongue-in-cheek.\n",
      "\n",
      "If playoff hockey is any more intense than the regular season varie\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 7942\n",
      "\n",
      "What is the policy regarding players and the minor league playoffs versus WC?\n",
      "I know that the Rangers are holding back Kovalev, Zubov, and Andersson for\n",
      "Binghamton, but I also know that the Whalers w\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in cossim_docs_idx:\n",
    "    print(f'Texto Número: {i}')\n",
    "    print_limited_strings(newsgroups_train.data[i], max_chars=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For some reason I never saw the original post on this thread, but if you are\n",
      "looking for fast polygon routines on vga on a PC, you really can't go past\n",
      "the mode X stuff from Dr Dobbs. This code is all p.domain (as far as I know),\n",
      "and in the original articles, the routines were all presented as dumb vga\n",
      "routines, and then optimised to modeX with some interesting discussion along\n",
      "the way.\n",
      "If you are interested, I could find out more details of the issues in question,\n",
      "(I have them at home).\n"
     ]
    }
   ],
   "source": [
    "idx = 3111\n",
    "print(newsgroups_train.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id de documentos similares: [ 3855 10996  2879  9623  6894]\n",
      "Valores de documentos similares: [0.26018123 0.24266591 0.22628022 0.22581871 0.21231708]\n"
     ]
    }
   ],
   "source": [
    "cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
    "cossim_values = np.sort(cossim)[::-1][1:6]\n",
    "cossim_docs_idx = np.argsort(cossim)[::-1][1:6]\n",
    "\n",
    "print(f'Id de documentos similares: {cossim_docs_idx}')\n",
    "print(f'Valores de documentos similares: {cossim_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase del documento original: comp.graphics\n",
      "Clase de documentos similares: ['comp.graphics', 'comp.sys.mac.hardware', 'comp.sys.ibm.pc.hardware', 'talk.politics.mideast', 'talk.politics.guns']\n"
     ]
    }
   ],
   "source": [
    "target_original = newsgroups_train.target_names[y_train[idx]]\n",
    "cossim_target_list = []\n",
    "for i in cossim_docs_idx:\n",
    "  cossim_target_list.append(newsgroups_train.target_names[y_train[i]])\n",
    "\n",
    "print(f'Clase del documento original: {target_original}')\n",
    "print(f'Clase de documentos similares: {cossim_target_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Número: 3855\n",
      "Hi there,\n",
      "\n",
      "I've made a VGA mode 13h graphics library available via FTP.  I originally\n",
      "wrote the routines as a kind of exercise for myself, but perhaps someone\n",
      "here will find them useful.  They are certainly useable as they are, but\n",
      "are missing some higher-level functionality.  They're intended more \n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 10996\n",
      "I have MANY questions for all you experts out there pertaining to apple's\n",
      "built-in video.  \n",
      "\n",
      "#1, Do all macs that have built-in video have the ability to use VGA monitors?\n",
      "#2, If so/if not, which macs have this capability?\n",
      "#3, Can they drive SVGA as well?\n",
      "#4, how big of a vga monitor can they drive?\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 2879\n",
      "Hi to all you PC gurus!\n",
      "\n",
      "I'm new to these groups and so please forgive me if my questions are frequently\n",
      "asked, but I don't know the answer :) I've been recently having some problems\n",
      "with my 386 computer with a Seagate 40 meg hard drive. I occasionally find\n",
      "corrupted files, but most of the time prog\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 9623\n",
      "Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
      "                 Prelude to Current Events in Nagorno-Karabakh\n",
      "\n",
      "        +---------------------------------------------------------+\n",
      "        |                                                         |\n",
      "        |  I saw a naked girl wi\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 6894\n",
      "Here is a press release from the White House.\n",
      "\n",
      " President Clinton's Remarks On Waco With Q/A\n",
      " To: National Desk\n",
      " Contact: White House Office of the Press Secretary, 202-456-2100\n",
      "\n",
      "   WASHINGTON, April 20 -- Following are remarks by President \n",
      "Clinton in a question and answer session with the press:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in cossim_docs_idx:\n",
    "    print(f'Texto Número: {i}')\n",
    "    print_limited_strings(newsgroups_train.data[i], max_chars=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documento 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stragegy of the government is interesting.  The real fear comes from\n",
      "them doing more than this.\n",
      "\n",
      "This is a voluntary program, and thus harder for us to object to on\n",
      "the surface.\n",
      "\n",
      "Their strategy is a business one rather than legal one.  They are\n",
      "pushing to get a standard in place, a secret standard, and if they\n",
      "get it as a standard then they will drive competitors out of the market.\n",
      "It will be legal to sell better, untapable encryption that doesn't have\n",
      "registered keys, but it will be difficult, and thus not a plan for\n",
      "most phone companies.\n",
      "\n",
      "You see, with clipper chip phones you'll be able to talk to any\n",
      "cellular company, or other phones or ports because they will follow\n",
      "the standard.  AT&T has already announced a clipper chip encryption\n",
      "product.  The government has marketed hard to get major vendors to\n",
      "use these chips.   If they get enough market share, they will rule.\n",
      "\n",
      "And thus there will be very little market for systems that can't be\n",
      "tapped by the police.  The public isn't that concerned about it now,\n",
      "after all.  They freely do calls that anybody with an old TV can listen\n",
      "to today!  They won't pay big extra bucks for proprietary phones that secure\n",
      "them only from the police.\n",
      "\n",
      "Well, some people will buy these phones, but they will only work with\n",
      "other proprietary phones, so the market will be small and the phones\n",
      "expensive.  Unless they are made in numbers large enough to sell them\n",
      "cheap, only the Mob will buy them.\n",
      "\n",
      "And this means that the FBI will want to track the customer lists of\n",
      "better encryption phones, because \"the only reason a person would want\n",
      "one is to evade the police.\"\n",
      "\n",
      "Interesting.\n"
     ]
    }
   ],
   "source": [
    "idx = 2954\n",
    "print(newsgroups_train.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id de documentos similares: [ 8534 10575  6806  2551  5856]\n",
      "Valores de documentos similares: [0.40508258 0.39032881 0.38777565 0.3780144  0.37469965]\n"
     ]
    }
   ],
   "source": [
    "cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
    "cossim_values = np.sort(cossim)[::-1][1:6]\n",
    "cossim_docs_idx = np.argsort(cossim)[::-1][1:6]\n",
    "\n",
    "print(f'Id de documentos similares: {cossim_docs_idx}')\n",
    "print(f'Valores de documentos similares: {cossim_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase del documento original: sci.crypt\n",
      "Clase de documentos similares: ['sci.crypt', 'sci.crypt', 'sci.crypt', 'sci.crypt', 'sci.crypt']\n"
     ]
    }
   ],
   "source": [
    "target_original = newsgroups_train.target_names[y_train[idx]]\n",
    "cossim_target_list = []\n",
    "for i in cossim_docs_idx:\n",
    "  cossim_target_list.append(newsgroups_train.target_names[y_train[i]])\n",
    "\n",
    "print(f'Clase del documento original: {target_original}')\n",
    "print(f'Clase de documentos similares: {cossim_target_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Número: 8534\n",
      "<If Clipper comes to cellular phones along with legal proscriptions against\n",
      "<using other cipher systems on these phones, a new and potentially dangerous\n",
      "<class of crime is created.\n",
      "\n",
      "Aside from possess\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 10575\n",
      "\n",
      "I am not an expert in the cryptography science, but some basic things\n",
      "seem evident to me, things which this Clinton Clipper do not address.\n",
      "The all pertain to opportunites for abuse, and conclusions \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 6806\n",
      "\n",
      "This isn't true.  Today's criminals regularly use all sorts of unsafe\n",
      "methods, from cordless phones to cellular phones to plain old copper\n",
      "wire analog phones that you can put alligator clips on to pl\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 2551\n",
      "\n",
      "\n",
      "If Brad's analysis is correct, it may offer an explanation for why the\n",
      "encryption algorithm is being kept secret.  This will prevent competitors\n",
      "from coming out with Clipper-compatible phones which \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 5856\n",
      "\n",
      "Thanks for posting this and making it available. This post will be LONG, I will\n",
      "comment on most of it, and am reluctantly leaving all of the original in place\n",
      "to provide context.\n",
      "\n",
      "Please note that an\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in cossim_docs_idx:\n",
    "    print(f'Texto Número: {i}')\n",
    "    print_limited_strings(newsgroups_train.data[i], max_chars=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documento 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I would guess not.  Dr. Neil Gehrels of CGRO is the son of Dr. Tom\n",
      "Gehrels of the University of Arizona.  Since he's long had research\n",
      "interests in asteroids and other solar-system astronomy, Tom is the\n",
      "one more likely to have discovered  a comet (and thus had his name\n",
      "attached to it).\n",
      "\n",
      "Tom Gehrels is a leader in the Spacewatch project, which has recently\n",
      "increased mankind's discovery rate on near-Earth asteroids (they're\n",
      "finding a couple every month).  For much more on this interesting guy,\n",
      "read his autobiography, *On a Glassy Sea*.\n"
     ]
    }
   ],
   "source": [
    "idx = 9759\n",
    "print(newsgroups_train.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id de documentos similares: [10594  1402  8613 10295  8610]\n",
      "Valores de documentos similares: [0.51540546 0.28077187 0.23025051 0.17873329 0.17773624]\n"
     ]
    }
   ],
   "source": [
    "cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
    "cossim_values = np.sort(cossim)[::-1][1:6]\n",
    "cossim_docs_idx = np.argsort(cossim)[::-1][1:6]\n",
    "\n",
    "print(f'Id de documentos similares: {cossim_docs_idx}')\n",
    "print(f'Valores de documentos similares: {cossim_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase del documento original: sci.space\n",
      "Clase de documentos similares: ['sci.space', 'sci.space', 'sci.space', 'sci.space', 'comp.os.ms-windows.misc']\n"
     ]
    }
   ],
   "source": [
    "target_original = newsgroups_train.target_names[y_train[idx]]\n",
    "cossim_target_list = []\n",
    "for i in cossim_docs_idx:\n",
    "  cossim_target_list.append(newsgroups_train.target_names[y_train[i]])\n",
    "\n",
    "print(f'Clase del documento original: {target_original}')\n",
    "print(f'Clase de documentos similares: {cossim_target_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Número: 10594\n",
      "\n",
      "Neil Gehrels is Prof. Tom Gehrels son.  Tom Gehrels was the discoverer\n",
      "of P/Gehrels 3 (as well as about 4 other comets - the latest of which\n",
      "does not bear his name, but rather the name \"Spacewatch\" since he was\n",
      "observing with that system when he found the latest comet).  \n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 1402\n",
      "\n",
      "\n",
      "\n",
      "No. I estimate a 99 % probability the Gehrels referred to\n",
      "is Thomas Gehrels of the Spacewatch project, Kitt Peak observatory.\n",
      "\n",
      "Maybe in the 24th century they could do gamma ray spectroscopy on\n",
      "distant asteroids with an orbiting observatory, but here in the\n",
      "primitive 20th we have to send a probe t\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 8613\n",
      "\n",
      "\n",
      "\n",
      "Thanks again.  One final question.  The name Gehrels wasn't known to\n",
      "me before this thread came up, but the May issue of Scientific American\n",
      "has an article about the \"Inconstant Cosmos\", with a photo of Neil\n",
      "Gehrels, project scientist for NASA's Compton Gamma Ray Observatory.\n",
      "Same person?\n",
      "-- \n",
      "Mar\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 10295\n",
      "Archive-name: space/math\n",
      "Last-modified: $Date: 93/04/01 14:39:12 $\n",
      "\n",
      "PERFORMING CALCULATIONS AND INTERPRETING DATA FORMATS\n",
      "\n",
      "    COMPUTING SPACECRAFT ORBITS AND TRAJECTORIES\n",
      "\n",
      "    References that have been frequently recommended on the net are:\n",
      "\n",
      "    \"Fundamentals of Astrodynamics\" Roger Bate, Donald Mu\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Texto Número: 8610\n",
      "I'm trying to find Tom Haapanen, formerly tom@wes.on.ca\n",
      "who was the keeper of the FAQ for this newsgroup.\n",
      "He was working at Watrerloo Engineering Software,\n",
      "but netfind can't even find that (but it may have\n",
      "been a uucp connection).  If anyone knows how to\n",
      "contact Tom, please let me know.\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in cossim_docs_idx:\n",
    "    print(f'Texto Número: {i}')\n",
    "    print_limited_strings(newsgroups_train.data[i], max_chars=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis de etiquetas:**\n",
    "1. Tras realizar el análisis de similaridad entre documentos, seleccionando 5 documentos al azar y comparándolos con el resto de la colección, se ha observado que, en la mayoría de los casos (documentos 1, 2, 4 y 5), los textos más similares presentan una coherencia significativa con la etiqueta de clasificación original. Esto sugiere que el modelo de similaridad utilizado es generalmente efectivo para identificar contenidos relacionados dentro de la misma categoría temática.\n",
    "2. Sin embargo, el análisis del tercer documento revela una excepción notable. En este caso, los textos más similares pertenecen a diferentes etiquetas de clasificación, algunas de ellas bastante divergentes en términos temáticos, como \"talk.politics.guns\" o \"talk.politics.mideast\". Esta discrepancia puede indicar la presencia de factores externos o una ambigüedad en el contenido que no fue captada por el modelo de similaridad.\n",
    "\n",
    "**Análisis de contenido del texto:**\n",
    "- _Documento 1:_\n",
    "Todos los documentos proporcionados comparten el tema central del béisbol y contienen discusiones sobre el desempeño, estadísticas y comparaciones entre jugadores. La similitud es más fuerte en los documentos 1 y 4 debido a la mención directa de jugadores comunes y debates similares. Los documentos 2, 3 y 5 también son similares en contexto y contenido, aunque se enfocan en diferentes aspectos y jugadores del deporte.\n",
    "\n",
    "- _Documento 2:_\n",
    "Los textos proporcionados comparten un tema central relacionado con el hockey, específicamente sobre decisiones y políticas respecto a la participación de jugadores en diferentes ligas y competencias internacionales. La discusión gira en torno a jugadores específicos, equipos como los Edmonton Oilers y los New York Rangers, y la controversia sobre si los jugadores deben participar en los playoffs de ligas menores o representar a sus países en campeonatos mundiales.\n",
    "\n",
    "- _Documento 3:_\n",
    "Los textos proporcionados muestran una clara división en cuanto a los temas discutidos. Los primeros tres textos están centrados en aspectos técnicos relacionados con computadoras, específicamente en gráficos VGA, programación y preguntas sobre hardware. Estos textos, junto con el original, son similares entre sí, ya que todos tratan de temas técnicos y de desarrollo informático.\n",
    "Sin embargo, los últimos dos textos (4 y 5) no tienen relación con el tema técnico de los primeros. El texto 4 parece abordar cuestiones de derechos humanos en un contexto geopolítico, mientras que el texto 5 es un comunicado de prensa de la Casa Blanca, lo cual es completamente distinto del enfoque técnico de los primeros documentos.\n",
    "\n",
    "- _Documento 4:_\n",
    "Los textos proporcionados comparten un enfoque común relacionado con la criptografía, la seguridad en las telecomunicaciones y las implicaciones del uso de tecnologías como el Clipper Chip. La mayoría de los textos discuten preocupaciones sobre la privacidad, la seguridad, la intervención del gobierno y las posibles consecuencias de implementar estándares de cifrado que podrían ser explotados o utilizados para fines de vigilancia.\n",
    "En conjunto, estos textos son similares en cuanto al tema central, ya que todos abordan aspectos técnicos y éticos de la criptografía y la seguridad en las comunicaciones. La preocupación sobre el control gubernamental y las implicaciones para la privacidad son temas recurrentes en estos documentos, lo que los hace altamente relacionados y coherentes entre sí.\n",
    "\n",
    "- _Documento 5:_\n",
    "Los textos proporcionados en su mayoría comparten un enfoque común relacionado con la figura de Tom Gehrels y sus contribuciones al campo de la astronomía, específicamente en el descubrimiento de cometas y su liderazgo en el proyecto Spacewatch. Los documentos discuten su legado, la conexión con su hijo Neil Gehrels, y mencionan su trabajo en la astronomía del sistema solar.\n",
    "Los textos 1 a 3 son altamente similares, ya que todos se centran en la misma persona y su trabajo en el ámbito astronómico. Los textos 4 y 5, aunque están relacionados con temas científicos y tecnológicos, se desvían significativamente del tema central de los primeros tres, enfocándose en cálculos matemáticos de órbitas y en la búsqueda de información de contacto de otra persona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clasificación Naïve Bayes\n",
    "- Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
    "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros de instanciación del vectorizador y los modelos\n",
    "- Probar modelos de Naïve Bayes Multinomial y ComplementNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un pipeline con TfidfVectorizer y MultinomialNB\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Definir el grid de parámetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'tfidf__max_df': [0.1, 0.3, 0.5],\n",
    "    'tfidf__ngram_range': [(1, 2)],\n",
    "    'tfidf__lowercase': [True, False],\n",
    "    'nb__alpha': np.arange(0.01, 0.016, 0.005),\n",
    "    'nb__fit_prior': [False]\n",
    "}\n",
    "\n",
    "# Crear el scorer para F1 con average='macro'\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Configurar el GridSearchCV\n",
    "multinomial = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2, scoring=f1_macro_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      " Mejores Parametros Encontrados:\n",
      "{'nb__alpha': np.float64(0.01), 'nb__fit_prior': False, 'tfidf__lowercase': True, 'tfidf__max_df': 0.1, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      " Mejor puntaje de cross-validation:\n",
      "0.7488635630581871\n"
     ]
    }
   ],
   "source": [
    "multinomial.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "print(\"\\n Mejores Parametros Encontrados:\")\n",
    "print(multinomial.best_params_)\n",
    "print(\"\\n Mejor puntaje de cross-validation:\")\n",
    "print(multinomial.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6750288007499659)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred =  multinomial.predict(newsgroups_test.data)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un pipeline con TfidfVectorizer y MultinomialNB\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "# Definir el grid de parámetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'tfidf__max_df': [0.1, 0.3, 0.5],\n",
    "    'tfidf__ngram_range': [(1, 2)],\n",
    "    'tfidf__lowercase': [True, False],\n",
    "    'nb__alpha': np.arange(0.01, 0.016, 0.005),\n",
    "    'nb__fit_prior': [False]\n",
    "}\n",
    "\n",
    "# Crear el scorer para F1 con average='macro'\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Configurar el GridSearchCV\n",
    "complement = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2, scoring=f1_macro_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      " Mejores Parametros Encontrados:\n",
      "{'nb__alpha': np.float64(0.015), 'nb__fit_prior': False, 'tfidf__lowercase': False, 'tfidf__max_df': 0.3, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      " Mejor puntaje de cross-validation:\n",
      "0.7621714472506199\n"
     ]
    }
   ],
   "source": [
    "complement.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "print(\"\\n Mejores Parametros Encontrados:\")\n",
    "print(complement.best_params_)\n",
    "print(\"\\n Mejor puntaje de cross-validation:\")\n",
    "print(complement.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6921568285995371)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred =  complement.predict(newsgroups_test.data)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenaron y evaluaron dos modelos de clasificación Naïve Bayes: Multinomial y ComplementNB, con el objetivo de maximizar el desempeño de clasificación (f1-score macro).\n",
    "El modelo ComplementNB mostró un rendimiento superior tanto en la validación cruzada como en el conjunto de test en comparación con el modelo Multinomial. \n",
    "\n",
    "En particular:\n",
    "- Multinomial Naïve Bayes alcanzó un puntaje de f1 en el conjunto de test de 0.6750 y un puntaje en validación cruzada de 0.7489.\n",
    "- ComplementNB logró un mejor puntaje de f1 en el conjunto de test de 0.6922 y un mejor puntaje en validación cruzada de 0.7622.\n",
    "\n",
    "Estos resultados indican que, bajo las configuraciones de parámetros seleccionadas, el modelo ComplementNB no solo generaliza mejor. \n",
    "\n",
    "La diferencia en los parámetros, como el uso de minúsculas y la proporción máxima de documentos (max_df), junto con un ajuste fino del parámetro alpha, ha contribuido a mejorar el desempeño de ComplementNB en este caso específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transponer matriz documento-término\n",
    "Transponer la matriz documento-término. De esa manera se obtiene una matriz término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
    "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Column sparse matrix of dtype 'float64'\n",
       "\twith 1103627 stored elements and shape (101631, 11314)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_x = X_train.T\n",
    "transposed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26461, 38788, 77274, 41101, 52493]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_words = ['century', 'explosion', 'religion', 'football', 'jobs']\n",
    "indexes = [tfidfvect.vocabulary_[word] for word in selected_words]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra: century\n",
      "Palabras similares: ['20th', 'autographs', '1909', '1883', 'homework']\n",
      "Valores de similitud de palabras: [0.40049998 0.3256703  0.30023232 0.29808301 0.25549192]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Palabra: explosion\n",
      "Palabras similares: ['ignited', 'leveled', 'elevator', 'grain', 'mehola']\n",
      "Valores de similitud de palabras: [0.68873205 0.61372846 0.58244308 0.49827317 0.31902375]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Palabra: religion\n",
      "Palabras similares: ['religious', 'religions', 'categorized', 'purpsoe', 'crusades']\n",
      "Valores de similitud de palabras: [0.24507616 0.2116499  0.20391513 0.20075281 0.19870798]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Palabra: football\n",
      "Palabras similares: ['madden', 'palying', 'zabriskie', 'sportstalk', 'basketball']\n",
      "Valores de similitud de palabras: [0.37407324 0.33418802 0.32561149 0.3172781  0.28963826]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Palabra: jobs\n",
      "Palabras similares: ['jobless', 'paring', 'toughen', 'chalk', 'filibustering']\n",
      "Valores de similitud de palabras: [0.45196649 0.44916672 0.36146637 0.31283009 0.29206519]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(selected_words):\n",
    "    print(f'Palabra: {word}')\n",
    "    cossim = cosine_similarity(transposed_x[indexes[i]], transposed_x)[0]\n",
    "    cossim_values = np.sort(cossim)[::-1][1:6]\n",
    "    cossim_words_idx = np.argsort(cossim)[::-1][1:6]\n",
    "    similar_words = [tfidfvect.get_feature_names_out()[idx] for idx in cossim_words_idx]\n",
    "    print(f'Palabras similares: {similar_words}')\n",
    "    print(f'Valores de similitud de palabras: {cossim_values}')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Palabra: \"century\"\n",
    "- Palabras similares: ['20th', 'autographs', '1909', '1883', 'homework']\n",
    "- Conclusión: Las palabras más similares a \"century\" están mayormente relacionadas con periodos de tiempo específicos ('20th', '1909', '1883'), lo cual es coherente dado que \"century\" es un término temporal. La presencia de 'autographs' sugiere una posible conexión con eventos históricos o coleccionismo, mientras que 'homework' parece ser menos directamente relacionado, quizás indicando un uso menos común o una interpretación más amplia del contexto en el que se encuentran estas palabras.\n",
    "\n",
    "2. Palabra: \"explosion\"\n",
    "- Palabras similares: ['ignited', 'leveled', 'elevator', 'grain', 'mehola']\n",
    "- Conclusión: Las palabras más similares a \"explosion\" son altamente coherentes en relación con eventos de explosiones y sus posibles causas o efectos. 'Ignited' y 'leveled' están directamente relacionados con la idea de una explosión. 'Elevator' y 'grain' pueden sugerir contextos como explosiones en silos de grano, mientras que 'mehola' parece ser un término menos interpretado, posiblemente relacionado con un evento específico o un lugar asociado a una explosión.\n",
    "\n",
    "3. Palabra: \"religion\"\n",
    "- Palabras similares: ['religious', 'religions', 'categorized', 'purpsoe', 'crusades']\n",
    "- Conclusión: Las palabras similares a \"religion\" son muy coherentes y refuerzan el tema. 'Religious' y 'religions' están directamente relacionadas, mientras que 'categorized' podría reflejar una organización o clasificación de religiones. 'Purpsoe' parece ser una variante ortográfica de 'purpose', que podría relacionarse con los objetivos o motivaciones de la religión, y 'crusades' conecta directamente con un evento histórico asociado a la religión.\n",
    "\n",
    "4. Palabra: \"football\"\n",
    "- Palabras similares: ['madden', 'palying', 'zabriskie', 'sportstalk', 'basketball']\n",
    "- Conclusión: Las palabras similares a \"football\" están claramente relacionadas con el deporte, en particular con referencias culturales ('madden', que podría referirse a los videojuegos de Madden NFL) y otros deportes ('basketball'). 'Palying' parece ser una variante de 'playing', relacionada con el acto de jugar al fútbol. 'Zabriskie' podría ser un apellido de un jugador o una referencia dentro del contexto deportivo, y 'sportstalk' sugiere discusiones sobre deportes, lo cual es coherente con la temática.\n",
    "\n",
    "5. Palabra: \"jobs\"\n",
    "- Palabras similares: ['jobless', 'paring', 'toughen', 'chalk', 'filibustering']\n",
    "- Conclusión: Las palabras similares a \"jobs\" se relacionan principalmente con el mercado laboral y los desafíos asociados. 'Jobless' está directamente relacionado con la falta de empleo. 'Paring' y 'toughen' podrían estar relacionados con acciones de ajuste o endurecimiento en el mercado laboral. 'Chalk' y 'filibustering', aunque menos evidentes, podrían estar conectados con discusiones políticas o educativas sobre empleo, o ser términos utilizados en contextos específicos dentro de este tema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
